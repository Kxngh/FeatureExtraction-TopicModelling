{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset \n",
    "\n",
    "Picking up class 10th CBSE book \"First Flight\", cause its a book with a lot of content which is relevel to this project. Converting that book into a dataset and then working further upon it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kxngh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing jeff101.pdf...\n",
      "Processing jeff102.pdf...\n",
      "Processing jeff103.pdf...\n",
      "Processing jeff104.pdf...\n",
      "Processing jeff105.pdf...\n",
      "Processing jeff106.pdf...\n",
      "Processing jeff107.pdf...\n",
      "Processing jeff108.pdf...\n",
      "Processing jeff109.pdf...\n",
      "Dataset compiled and saved to compiled_chapters_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'([.?!])\\s+', r'\\1\\n', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "pdf_directory = 'jeff1dd'\n",
    "pdf_files = glob.glob(os.path.join(pdf_directory, '*.pdf'))\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    chapter_name = os.path.basename(pdf_file)\n",
    "    print(f\"Processing {chapter_name}...\")\n",
    "    raw_text = extract_text_from_pdf(pdf_file)\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    sentences = tokenize_sentences(cleaned_text)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        all_data.append({'chapter': chapter_name, 'sentence': sentence})\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('compiled_chapters_dataset.csv', index=False) \n",
    "print(\"Dataset compiled and saved to compiled_chapters_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kxngh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/kxngh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 285 potential task sentences out of 2653 total sentences.\n",
      "        chapter                                           sentence  is_task\n",
      "1   jeff101.pdf               But what should we put our faith in?     True\n",
      "6   jeff101.pdf  Think what your answers to these questions wou...     True\n",
      "8   jeff101.pdf  Have you ever sent or received money in this way?     True\n",
      "23  jeff101.pdf  Use a part of your pocket money, and submit th...     True\n",
      "24  jeff101.pdf  See how your partner enjoys getting money by p...     True\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "task_keywords = [\"has to\", \"need to\", \"must\", \"should\", \"please\", \"kindly\", \"required to\", \"action\", \"task\"]\n",
    "\n",
    "def is_task_sentence(sentence):\n",
    "    sentence_lower = sentence.lower()\n",
    "    if any(keyword in sentence_lower for keyword in task_keywords):\n",
    "        return True\n",
    "    \n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    if tokens:\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        if pos_tags[0][1].startswith(\"VB\"):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "df = pd.read_csv(\"compiled_chapters_dataset.csv\")\n",
    "df['is_task'] = df['sentence'].apply(is_task_sentence)\n",
    "\n",
    "task_df = df[df['is_task']]\n",
    "\n",
    "task_df.to_csv(\"extracted_tasks_dataset.csv\", index=False)\n",
    "\n",
    "print(f\"Extracted {len(task_df)} potential task sentences out of {len(df)} total sentences.\")\n",
    "print(task_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0 samples:\n",
      "•Guided them through the reading activity by pr...\n",
      "Activit y: Before filling out the form, get the...\n",
      "Give them enough time to read, and then discuss...\n",
      "                                Let freedom reign.\n",
      "At first, as a student, I wanted freedom only f...\n",
      "\n",
      "Cluster 1 samples:\n",
      "              But what should we put our faith in?\n",
      " Have you ever sent or received money in this way?\n",
      "Use a part of your pocket money, and submit the...\n",
      "See how your partner enjoys getting money by post!\n",
      "(i)In addition to the sender , the for m has to...\n",
      "\n",
      "Cluster 2 samples:\n",
      "When he finished, he went to the window to buy ...\n",
      "Join the sentences given below using who, whom ...\n",
      "Given below is the passage for listening activi...\n",
      "Given below are sentences carrying one part of ...\n",
      "                              Does he arrive safe?\n",
      "\n",
      "Cluster 3 samples:\n",
      "Think what your answers to these questions woul...\n",
      "Use the ‘Oral Comprehension Checks’ in the appr...\n",
      "Check your guess with this news item (from the ...\n",
      "Make a guess and match each expression with an ...\n",
      "Used in this way with the and/or in the plural,...\n",
      "\n",
      "Categorized tasks saved to 'categorized_tasks_dataset.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kxngh/Desktop/Wasted Potential /Internship assignments/vijayi/.venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df_tasks = pd.read_csv(\"extracted_tasks_dataset.csv\")\n",
    "\n",
    "task_sentences = df_tasks['sentence'].tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(task_sentences)\n",
    "\n",
    "n_clusters = 4\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "df_tasks['cluster'] = kmeans.labels_\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    print(f\"\\nCluster {i} samples:\")\n",
    "    print(df_tasks[df_tasks['cluster'] == i]['sentence'].head(5).to_string(index=False))\n",
    "\n",
    "df_tasks.to_csv(\"categorized_tasks_dataset.csv\", index=False)\n",
    "print(\"\\nCategorized tasks saved to 'categorized_tasks_dataset.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kxngh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/kxngh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/kxngh/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/kxngh/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2653 sentences. Found 285 potential task sentences.\n",
      "Sample output:\n",
      "        chapter                                           sentence  is_task  \\\n",
      "1   jeff101.pdf               But what should we put our faith in?     True   \n",
      "6   jeff101.pdf  Think what your answers to these questions wou...     True   \n",
      "8   jeff101.pdf  Have you ever sent or received money in this way?     True   \n",
      "23  jeff101.pdf  Use a part of your pocket money, and submit th...     True   \n",
      "24  jeff101.pdf  See how your partner enjoys getting money by p...     True   \n",
      "\n",
      "   agent deadline  \n",
      "1   None     None  \n",
      "6   None     None  \n",
      "8   None     None  \n",
      "23  None     None  \n",
      "24  None     None  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "task_keywords = [\"has to\", \"need to\", \"must\", \"should\", \"please\", \"kindly\", \"required to\", \"action\", \"task\"]\n",
    "\n",
    "def is_task_sentence(sentence):\n",
    "    sentence_lower = sentence.lower()\n",
    "    if any(keyword in sentence_lower for keyword in task_keywords):\n",
    "        return True\n",
    "\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    if tokens:\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        if pos_tags[0][1].startswith(\"VB\"):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def extract_agent(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    for word, tag in pos_tags:\n",
    "        if tag in ['NNP', 'NNPS']:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "def extract_deadline(sentence):\n",
    "    time_pattern = r'\\bby\\s+(\\d{1,2}(?::\\d{2})?\\s*(am|pm))\\b'\n",
    "    match = re.search(time_pattern, sentence, flags=re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    \n",
    "    deadline_keywords = [\"tomorrow\", \"today\", \"next week\", \"by the end of day\"]\n",
    "    for keyword in deadline_keywords:\n",
    "        if keyword in sentence.lower():\n",
    "            return keyword\n",
    "    return None\n",
    "\n",
    "df = pd.read_csv(\"compiled_chapters_dataset.csv\")\n",
    "\n",
    "def process_sentence(row):\n",
    "    sentence = row['sentence']\n",
    "    task_flag = is_task_sentence(sentence)\n",
    "    agent = extract_agent(sentence) if task_flag else None\n",
    "    deadline = extract_deadline(sentence) if task_flag else None\n",
    "    return pd.Series({'is_task': task_flag, 'agent': agent, 'deadline': deadline})\n",
    "\n",
    "task_details = df.apply(process_sentence, axis=1)\n",
    "df = pd.concat([df, task_details], axis=1)\n",
    "\n",
    "df_tasks = df[df['is_task'] == True].copy()\n",
    "\n",
    "output_csv = \"extracted_tasks_with_details.csv\"\n",
    "df_tasks.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Processed {len(df)} sentences. Found {len(df_tasks)} potential task sentences.\")\n",
    "print(\"Sample output:\")\n",
    "print(df_tasks.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kxngh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.007*\"say\" + 0.007*\"tickets\" + 0.007*\"read\" + 0.006*\"students\" + 0.006*\"tiger\"\n",
      "Topic 1: 0.009*\"something\" + 0.008*\"know\" + 0.008*\"said\" + 0.006*\"please\" + 0.006*\"given\"\n",
      "Topic 2: 0.016*\"must\" + 0.007*\"bus\" + 0.007*\"get\" + 0.005*\"reprint\" + 0.005*\"please\"\n",
      "Topic 3: 0.012*\"read\" + 0.009*\"think\" + 0.009*\"must\" + 0.005*\"story\" + 0.005*\"one\"\n",
      "Categorized tasks saved to categorized_tasks_with_LDA.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "df_tasks = pd.read_csv(\"extracted_tasks_with_details.csv\")\n",
    "task_sentences = df_tasks['sentence'].tolist()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "processed_docs = [preprocess_text(sentence) for sentence in task_sentences]\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "num_topics = 4\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, \n",
    "                     random_state=42, passes=10)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(num_words=5):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "def assign_topic(text):\n",
    "    tokens = preprocess_text(text)\n",
    "    bow = dictionary.doc2bow(tokens)\n",
    "    topic_probs = lda_model.get_document_topics(bow)\n",
    "    if topic_probs:\n",
    "        top_topic = max(topic_probs, key=lambda x: x[1])[0]\n",
    "        return top_topic\n",
    "    return None\n",
    "\n",
    "df_tasks['category'] = df_tasks['sentence'].apply(assign_topic)\n",
    "\n",
    "output_csv = \"categorized_tasks_with_LDA.csv\"\n",
    "df_tasks.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Categorized tasks saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kxngh/Desktop/Wasted Potential /Internship assignments/vijayi/.venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0 sample sentences:\n",
      "              But what should we put our faith in?\n",
      "Think what your answers to these questions woul...\n",
      " Have you ever sent or received money in this way?\n",
      "Use a part of your pocket money, and submit the...\n",
      "See how your partner enjoys getting money by post!\n",
      "\n",
      "\n",
      "\n",
      "Cluster 1 sample sentences:\n",
      "  Do you think a crow is often mentioned in poems?\n",
      "Read the poem silently once, and say which stan...\n",
      "He should be lurking in shadow, Sliding through...\n",
      "He should be snarling around houses At the jung...\n",
      "LESLIE NORRIS Reprint 2024-25 snarls: makes an ...\n",
      "\n",
      "\n",
      "\n",
      "Cluster 2 sample sentences:\n",
      "Join the sentences given below using who, whom ...\n",
      "IV.Using Negatives for Emphasis We know that se...\n",
      "     Try to say what qualities are being compared.\n",
      "•Guided them through the reading activity by pr...\n",
      "•Provided interesting exercises to strengthen s...\n",
      "\n",
      "\n",
      "\n",
      "Cluster 3 sample sentences:\n",
      "Have you experienced a similar moment that chan...\n",
      "                                Let freedom reign.\n",
      "                         Choose the right answer .\n",
      "                              Does he arrive safe?\n",
      "                  Make it as humorous as possible.\n",
      "\n",
      "\n",
      "Categorized tasks saved to 'categorized_tasks_with_bert_4clusters.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df_tasks = pd.read_csv(\"extracted_tasks_with_details.csv\")\n",
    "task_sentences = df_tasks['sentence'].tolist()\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(task_sentences)\n",
    "\n",
    "n_clusters = 4\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(embeddings)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "df_tasks['category'] = labels\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    print(f\"\\nCluster {i} sample sentences:\")\n",
    "    print(df_tasks[df_tasks['category'] == i]['sentence'].head(5).to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "\n",
    "output_csv = \"categorized_tasks_with_bert_4clusters.csv\"\n",
    "df_tasks.to_csv(output_csv, index=False)\n",
    "print(f\"Categorized tasks saved to '{output_csv}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 0:\n",
    "This cluster appears to group sentences that are reflective or discussion-oriented. They often pose questions (e.g., “But what should we put our faith in?”) or prompt the reader to think (e.g., “Think what your answers to these questions would be…”). These might be more about prompting discussion or reflection rather than direct action commands.\n",
    "\n",
    "Cluster 1:\n",
    "The sentences here include descriptive and creative language (“He should be lurking in shadow, sliding through…” and “He should be snarling around houses…”) along with literary analysis questions (e.g., “Do you think a crow is often mentioned in poems?”). This cluster could represent tasks that involve creative description or critical analysis of text.\n",
    "\n",
    "Cluster 2:\n",
    "This group contains more structured instructions or directives—for example, tasks like “Join the sentences given below using who, whom…” and directives related to language exercises (“Try to say what qualities are being compared…”). It seems to capture more formal, exercise-like task instructions.\n",
    "\n",
    "Cluster 3:\n",
    "The sentences here are short, direct, and imperative (e.g., “Choose the right answer.”, “Make it as humorous as possible.”). They are clear commands or directives and could be seen as straightforward task instructions that require immediate action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation F1 Scores: [0.84848485 0.79020979 0.7766323  0.80565371 0.82068966]\n",
      "Mean CV F1 Score: 0.8083340613039802\n",
      "\n",
      "Detailed Evaluation Metrics:\n",
      "Accuracy: 0.9067\n",
      "Precision: 0.9321\n",
      "Recall: 0.7947\n",
      "F1 Score: 0.8580\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.93       346\n",
      "    positive       0.93      0.79      0.86       190\n",
      "\n",
      "    accuracy                           0.91       536\n",
      "   macro avg       0.91      0.88      0.89       536\n",
      "weighted avg       0.91      0.91      0.90       536\n",
      "\n",
      "\n",
      "Bootstrap Confidence Intervals:\n",
      "Accuracy: 0.9073 (95% CI: [0.88059701 0.93283582])\n",
      "Precision: 0.9312 (95% CI: [0.89022806 0.96987952])\n",
      "Recall: 0.7924 (95% CI: [0.73429309 0.84537166])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "class AdvancedTextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        cleaned_tokens = [\n",
    "            self.lemmatizer.lemmatize(token) \n",
    "            for token in tokens \n",
    "            if token not in self.stop_words and len(token) > 1\n",
    "        ]\n",
    "        return ' '.join(cleaned_tokens)\n",
    "\n",
    "df = pd.read_csv(\"csv_data.csv\")\n",
    "\n",
    "preprocessor = AdvancedTextPreprocessor()\n",
    "df['clean_review'] = df['review'].apply(preprocessor.preprocess)\n",
    "df['sentiment'] = df['sentiment'].str.strip().str.lower()\n",
    "df['sentiment'] = df['sentiment'].replace({\"postive\": \"positive\"})\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['sentiment_encoded'] = le.fit_transform(df['sentiment'])\n",
    "\n",
    "X = df['clean_review']\n",
    "y = df['sentiment_encoded']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "base_classifiers = [\n",
    "    ('lr', LogisticRegression(max_iter=1000, class_weight='balanced', C=0.1)),\n",
    "    ('svm', SVC(probability=True, class_weight='balanced', kernel='rbf')),\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=10, class_weight='balanced')),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=250, learning_rate=0.05, max_depth=5))\n",
    "]\n",
    "\n",
    "stacking_classifier = StackingClassifier(\n",
    "    estimators=base_classifiers,\n",
    "    final_estimator=LogisticRegression(max_iter=1000),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        stop_words='english', \n",
    "        ngram_range=(1,3),\n",
    "        max_features=8000,\n",
    "        sublinear_tf=True,\n",
    "        max_df=0.8,\n",
    "        min_df=2\n",
    "    )),\n",
    "    ('classifier', stacking_classifier)\n",
    "])\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='f1')\n",
    "\n",
    "print(\"Cross-validation F1 Scores:\", cv_scores)\n",
    "print(\"Mean CV F1 Score:\", cv_scores.mean())\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "y_test_labels = le.inverse_transform(y_test)\n",
    "y_pred_labels = le.inverse_transform(y_pred)\n",
    "\n",
    "print(\"\\nDetailed Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_labels, y_pred_labels):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test_labels, y_pred_labels, pos_label='positive'):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test_labels, y_pred_labels, pos_label='positive'):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test_labels, y_pred_labels, pos_label='positive'):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred_labels))\n",
    "\n",
    "def bootstrap_metric(y_true, y_pred, metric_func, n_iterations=1000, alpha=0.05):\n",
    "    metrics = []\n",
    "    for _ in range(n_iterations):\n",
    "        indices = np.random.randint(0, len(y_true), len(y_true))\n",
    "        sample_true = y_true[indices]\n",
    "        sample_pred = y_pred[indices]\n",
    "        metrics.append(metric_func(sample_true, sample_pred))\n",
    "    confidence_interval = np.percentile(metrics, [alpha/2*100, (1-alpha/2)*100])\n",
    "    return np.mean(metrics), confidence_interval\n",
    "\n",
    "accuracy_mean, accuracy_ci = bootstrap_metric(\n",
    "    np.array(y_test_labels), \n",
    "    np.array(y_pred_labels), \n",
    "    accuracy_score\n",
    ")\n",
    "precision_mean, precision_ci = bootstrap_metric(\n",
    "    np.array(y_test_labels), \n",
    "    np.array(y_pred_labels), \n",
    "    lambda y_t, y_p: precision_score(y_t, y_p, pos_label='positive')\n",
    ")\n",
    "recall_mean, recall_ci = bootstrap_metric(\n",
    "    np.array(y_test_labels), \n",
    "    np.array(y_pred_labels), \n",
    "    lambda y_t, y_p: recall_score(y_t, y_p, pos_label='positive')\n",
    ")\n",
    "\n",
    "print(\"\\nBootstrap Confidence Intervals:\")\n",
    "print(f\"Accuracy: {accuracy_mean:.4f} (95% CI: {accuracy_ci})\")\n",
    "print(f\"Precision: {precision_mean:.4f} (95% CI: {precision_ci})\")\n",
    "print(f\"Recall: {recall_mean:.4f} (95% CI: {recall_ci})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters saved: positive_reviews.csv and negative_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "# Save positive and negative clusters into separate CSV files\n",
    "positive_reviews = df[df['sentiment_encoded'] == le.transform(['positive'])[0]]\n",
    "negative_reviews = df[df['sentiment_encoded'] == le.transform(['negative'])[0]]\n",
    "\n",
    "positive_reviews.to_csv(\"positive_reviews.csv\", index=False)\n",
    "negative_reviews.to_csv(\"negative_reviews.csv\", index=False)\n",
    "\n",
    "print(\"Clusters saved: positive_reviews.csv and negative_reviews.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
